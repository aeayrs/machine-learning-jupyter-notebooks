{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building machine learning pipelines**\n",
    "\n",
    "The scikit-learn library has provisions to build machine learning pipelines. We just need to\n",
    "specify the functions, and it will build a composed object that makes the data go through the\n",
    "whole pipeline. This pipeline can include functions, such as preprocessing, feature selection,\n",
    "supervised learning, unsupervised learning, and so on. In this recipe, we will be building a\n",
    "pipeline to take the input feature vector, select the top *k* features, and then classify them\n",
    "using a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data\n",
    "# This will generate 20-dimensional feature vectors (n_features=20)\n",
    "X, y = make_classification(n_informative=4, n_features=20, n_redundant=0, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first step of the pipeline is to select the k best features\n",
    "# and before the datapoint is used further.\n",
    "# In this case, let's set k to 10.\n",
    "# Feature selector\n",
    "selector_k_best = SelectKBest(f_regression, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to use a random forest classifier to classify the data:\n",
    "# Random forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=50, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now ready to build the pipeline. The pipeline method\n",
    "# allows us to use predefined objects to build the pipeline\n",
    "\n",
    "# Build the machine learning pipeline\n",
    "pipeline_classifier = Pipeline([('selector', selector_k_best),\n",
    "                                ('rf', classifier)])\n",
    "\n",
    "# We can also assigned names to the blocks in out pipeline. In the preceding\n",
    "# line, we assign the selector name to our feature selector and\n",
    "# the rf to our random forest classifier. You are free to use any\n",
    "# other random names here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('selector',\n",
       "                 SelectKBest(k=6,\n",
       "                             score_func=<function f_regression at 0x116f61050>)),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=4, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=25, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also update these parameters as we go along. We can set\n",
    "# the parameters using the names that we assigned in the previous\n",
    "# step. For example, if we want to set k to 6 in the feature\n",
    "# selector and set n_estimators to 25 in the random forest\n",
    "# classifier, we can do it like in the following code. Not that\n",
    "# these are the variable names given in the previous step:\n",
    "\n",
    "pipeline_classifier.set_params(selector__k=6, rf__n_estimators=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('selector',\n",
       "                 SelectKBest(k=6,\n",
       "                             score_func=<function f_regression at 0x116f61050>)),\n",
       "                ('rf',\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=4, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=25, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's go ahead and train the classifier\n",
    "\n",
    "pipeline_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: \n",
      "[1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's predict the outputs for the training data:\n",
    "prediction = pipeline_classifier.predict(X)\n",
    "print(\"Predictions: \")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (0-indexed):  0, 5, 9, 10, 11, 15\n"
     ]
    }
   ],
   "source": [
    "# Print the selected features chosen by the selector\n",
    "features_status = pipeline_classifier.named_steps['selector'].get_support()\n",
    "selected_features = []\n",
    "for count, item in enumerate(features_status):\n",
    "    if item:\n",
    "        selected_features.append(count)\n",
    "\n",
    "print(\"Selected features (0-indexed): \", ', '.join([str(x) for x in selected_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works...**\n",
    "\n",
    "The advantage of selecting the *k* best features is that we will be able to work with low-dimensional\n",
    "data. This is helpful in reducing the computational complexity. The way in which\n",
    "we select the *k* best features is based on univariate feature selection. This performs univariate\n",
    "statistical tests and the extracts the top performing features from the feature vector. Univariate\n",
    "statistical tests refer to analysis techniques wher a single variable is involved.\n",
    "\n",
    "Once these tests are performed, each feature in the feature vector is assigned a score.\n",
    "Based on these scores, we select the top *k* features. We do this as a preprocessing step in\n",
    "our classifier pipeline. Once we extract the top *k* features, a *k*-dimensional feature vector is\n",
    "formed, and we use it as the input training data for the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
